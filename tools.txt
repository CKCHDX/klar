Layer	Purpose	Tools/Tech
1. Web Crawler	Crawl Swedish websites and collect data	Scrapy, Requests, BeautifulSoup, Selenium
2. Indexer & Storage	Store crawled data in a searchable index	Elasticsearch or Typesense or Meilisearch
3. Search Engine API (Backend)	Handle search queries and return results	Python + Flask or FastAPI
4. Frontend UI	Clean, fast, browser-based search interface	HTML/CSS, Vanilla JS or React
5. Database (Metadata, Logs, Config)	Store crawl metadata, query logs, etc.	PostgreSQL or SQLite
6. Infrastructure / Hosting	Run your search engine live	Docker, DigitalOcean, Hetzner, or AWS

ğŸ’» 1. Web Crawler

a custom crawler to crawl Swedish content.

Tools:

Scrapy (Python): Best for scalable, structured crawling.

BeautifulSoup (Python): For light, flexible HTML parsing.

Selenium: For JavaScript-heavy sites (only when needed).

tldextract: To restrict crawling to .se domains.

Output:

Crawled pages (HTML)

Extracted content: title, body text, metadata, language, URL, date

Example workflow:

Next Steps: Phase 1 Plan

âœ… Define Scope (done â€” Sweden only, Klar)

ğŸ§± Set up Crawler

ğŸ“¦ Set up Index/Search Engine

ğŸ§  Add basic ranking

ğŸ§ª Build API

ğŸ¨ Build Frontend UI

â˜ï¸ Host on test server

ğŸ§ª MVP Testing with sample domains

ğŸ” Improve based on real use

ğŸ“¢ Launch beta
