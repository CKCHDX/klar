"""
Klar 3.1+ Storage System
File-based persistent storage (no database required)
Stores crawled data in JSON files organized by domain
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import hashlib


class FileStorage:
    """File-based persistent storage for crawled content"""
    
    def __init__(self, data_path: Path):
        self.data_path = Path(data_path)
        self.data_path.mkdir(parents=True, exist_ok=True)
        
        # Create subdirectories
        (self.data_path / 'pages').mkdir(exist_ok=True)
        (self.data_path / 'index').mkdir(exist_ok=True)
        (self.data_path / 'metadata').mkdir(exist_ok=True)
        
        print(f"[Storage] ðŸ“ Initialized file-based storage at: {self.data_path}")
    
    def save_page(self, page_data: Dict) -> bool:\n        \"\"\"Save crawled page to file\"\"\"\n        try:\n            url = page_data.get('url', '')\n            domain = page_data.get('domain', 'unknown')\n            \n            # Create domain directory\n            domain_path = self.data_path / 'pages' / domain\n            domain_path.mkdir(parents=True, exist_ok=True)\n            \n            # Generate filename from URL hash\n            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]\n            filename = f\"{url_hash}.json\"\n            filepath = domain_path / filename\n            \n            # Save page data\n            with open(filepath, 'w', encoding='utf-8') as f:\n                json.dump(page_data, f, indent=2, ensure_ascii=False)\n            \n            # Update index\n            self._update_index(page_data)\n            \n            print(f\"[Storage] âœ“ Saved: {domain}/{filename}\")\n            return True\n        \n        except Exception as e:\n            print(f\"[Storage] âœ— Error saving page: {e}\")\n            return False\n    \n    def save_batch(self, pages: List[Dict]) -> int:\n        \"\"\"Save multiple pages, return count\"\"\"\n        saved = 0\n        for page in pages:\n            if self.save_page(page):\n                saved += 1\n        return saved\n    \n    def get_page(self, url: str) -> Optional[Dict]:\n        \"\"\"Retrieve page by URL\"\"\"\n        try:\n            # Search for the URL in index\n            index_path = self.data_path / 'index' / 'url_index.json'\n            if index_path.exists():\n                with open(index_path, 'r', encoding='utf-8') as f:\n                    url_index = json.load(f)\n                \n                if url in url_index:\n                    filepath = self.data_path / url_index[url]\n                    if filepath.exists():\n                        with open(filepath, 'r', encoding='utf-8') as f:\n                            return json.load(f)\n            \n            return None\n        \n        except Exception as e:\n            print(f\"[Storage] âœ— Error retrieving page: {e}\")\n            return None\n    \n    def search_pages(self, query: str, domain: Optional[str] = None) -> List[Dict]:\n        \"\"\"Search pages by query (simple keyword search)\"\"\"\n        results = []\n        query_lower = query.lower()\n        \n        try:\n            # Search in domain or all domains\n            if domain:\n                search_path = self.data_path / 'pages' / domain\n                if not search_path.exists():\n                    return results\n                domain_paths = [search_path]\n            else:\n                domain_paths = list((self.data_path / 'pages').glob('*'))\n            \n            # Search through pages\n            for domain_path in domain_paths:\n                if not domain_path.is_dir():\n                    continue\n                \n                for page_file in domain_path.glob('*.json'):\n                    try:\n                        with open(page_file, 'r', encoding='utf-8') as f:\n                            page = json.load(f)\n                        \n                        # Check if query matches title, description, or content\n                        content = (\n                            page.get('title', '') + ' ' +\n                            page.get('description', '') + ' ' +\n                            page.get('content', '')[:500]\n                        ).lower()\n                        \n                        if query_lower in content:\n                            results.append(page)\n                    \n                    except:\n                        pass\n            \n            print(f\"[Storage] ðŸ” Found {len(results)} matches for '{query}'\")\n            return results\n        \n        except Exception as e:\n            print(f\"[Storage] âœ— Search error: {e}\")\n            return results\n    \n    def get_domain_stats(self, domain: str) -> Dict:\n        \"\"\"Get statistics for a domain\"\"\"\n        try:\n            domain_path = self.data_path / 'pages' / domain\n            \n            if not domain_path.exists():\n                return {'domain': domain, 'pages': 0, 'size_bytes': 0}\n            \n            pages = list(domain_path.glob('*.json'))\n            total_size = sum(f.stat().st_size for f in pages)\n            \n            return {\n                'domain': domain,\n                'pages': len(pages),\n                'size_bytes': total_size,\n                'size_mb': round(total_size / (1024 * 1024), 2)\n            }\n        \n        except Exception as e:\n            print(f\"[Storage] âœ— Error getting stats: {e}\")\n            return {'domain': domain, 'pages': 0, 'size_bytes': 0}\n    \n    def get_all_domains(self) -> List[str]:\n        \"\"\"Get list of all domains with data\"\"\"\n        try:\n            pages_path = self.data_path / 'pages'\n            if not pages_path.exists():\n                return []\n            \n            domains = [d.name for d in pages_path.iterdir() if d.is_dir()]\n            return sorted(domains)\n        \n        except Exception as e:\n            print(f\"[Storage] âœ— Error getting domains: {e}\")\n            return []\n    \n    def _update_index(self, page_data: Dict):\n        \"\"\"Update URL index\"\"\"\n        try:\n            index_path = self.data_path / 'index' / 'url_index.json'\n            \n            # Load existing index\n            url_index = {}\n            if index_path.exists():\n                with open(index_path, 'r', encoding='utf-8') as f:\n                    url_index = json.load(f)\n            \n            # Add new entry\n            url = page_data.get('url', '')\n            domain = page_data.get('domain', 'unknown')\n            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]\n            \n            filepath = f\"pages/{domain}/{url_hash}.json\"\n            url_index[url] = filepath\n            \n            # Save updated index\n            with open(index_path, 'w', encoding='utf-8') as f:\n                json.dump(url_index, f, indent=2)\n        \n        except Exception as e:\n            print(f\"[Storage] âš ï¸  Error updating index: {e}\")\n    \n    def get_storage_info(self) -> Dict:\n        \"\"\"Get overall storage information\"\"\"\n        try:\n            total_size = 0\n            total_files = 0\n            \n            for root, dirs, files in os.walk(self.data_path / 'pages'):\n                for file in files:\n                    if file.endswith('.json'):\n                        filepath = os.path.join(root, file)\n                        total_size += os.path.getsize(filepath)\n                        total_files += 1\n            \n            return {\n                'total_pages': total_files,\n                'total_size_bytes': total_size,\n                'total_size_mb': round(total_size / (1024 * 1024), 2),\n                'domains': len(self.get_all_domains()),\n                'storage_path': str(self.data_path)\n            }\n        \n        except Exception as e:\n            print(f\"[Storage] âœ— Error getting storage info: {e}\")\n            return {'total_pages': 0, 'total_size_bytes': 0}\n    \n    def delete_domain_data(self, domain: str) -> bool:\n        \"\"\"Delete all data for a domain\"\"\"\n        try:\n            import shutil\n            domain_path = self.data_path / 'pages' / domain\n            \n            if domain_path.exists():\n                shutil.rmtree(domain_path)\n                print(f\"[Storage] ðŸ—‘ï¸  Deleted data for {domain}\")\n                return True\n            \n            return False\n        \n        except Exception as e:\n            print(f\"[Storage] âœ— Error deleting domain: {e}\")\n            return False\n    \n    def export_domain(self, domain: str, export_path: Path) -> bool:\n        \"\"\"Export domain data to JSON file\"\"\"\n        try:\n            domain_path = self.data_path / 'pages' / domain\n            \n            if not domain_path.exists():\n                return False\n            \n            pages = []\n            for page_file in domain_path.glob('*.json'):\n                with open(page_file, 'r', encoding='utf-8') as f:\n                    pages.append(json.load(f))\n            \n            export_path.parent.mkdir(parents=True, exist_ok=True)\n            with open(export_path, 'w', encoding='utf-8') as f:\n                json.dump({\n                    'domain': domain,\n                    'exported': datetime.now().isoformat(),\n                    'page_count': len(pages),\n                    'pages': pages\n                }, f, indent=2, ensure_ascii=False)\n            \n            print(f\"[Storage] ðŸ’¾ Exported {len(pages)} pages to {export_path}\")\n            return True\n        \n        except Exception as e:\n            print(f\"[Storage] âœ— Export error: {e}\")\n            return False\n